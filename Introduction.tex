%------------------------------------------------------------------------
\chapter{Introduction}
\label{Introduction.ch}
%------------------------------------------------------------------------
 Object detection is one of the most common problems in computer vision, and in
most cases, solutions for it are complicated or/and expensive. Therefore finding
ways to bring this cost or complexity down can be very valuable. This costly
problem is a straightforward task for us as humans, so looking into the methods
humans use to find an object or recognize it can assist in proposing a solution
for an AI system.
 
When a human is looking for an object, based on experience, he limits the
search area to places he expects to find the object of interest. If the search
area is a familiar place, based on their memory, they directly go to the last place
they saw the object. If it is a new place and no prior knowledge about that place
is available, he looks for places that logically the object should be located at.

This can be easier to imagine if we assume that the room is dark and our
agent needs to touch objects to find them. For instance, if the object is a Telephone, the agent first look for a table or a desk in the room and then start
moving his hand slowly on the desk/table's surface (context for the object of
interest) to find the object that is the most similar to what he has in memory of
a Telephone (object detection). Only in the worst-case scenario and without the
knowledge of the object one would perform a thorough search of the whole
scene. In other words, the strategy depends on the amount of information about
the target object.



{\it Torralba et al.} in \cite{eyeMovement} pointed out that many experiments show
that the human visual system uses contextual information to facilitate a search
for objects. They propose the use of context for object detection using 2D im-
ages. They take two parallel approaches to predict the image regions likely to
be focused on by human observers, naturally searching for objects in real-world
scenes; One computes local features (saliency), and the other computes global
(scene-centered) features.
%Although, it is also mentioned that how this information is applied in the process is not completely known to scientists yet.

%To clarify what is considered as object context in this work, it should be mentioned that surfaces that surround an object as 
%a single body is a context for the object. 
A similar behavior could be implemented on a mobile robot or any AI system. A piece of valuable prior knowledge for an object detection/recognition
problem could be semantic knowledge about places and contexts where the object of interest can be found. This can reduce the search time and increase the chance of success and correctness of the results. The role of
contextual information is even more significant when available direct object features are not reliable enough due to viewing conditions. 

%------------------------------------------------------------------------------------------------------------

\section {Outline}
\label{Outline.sec}

In the following sections of this chapter, the general formulation of the problem is stated, follows by a brief analysis of related works. Then motivation and our contributions are presented.
% In Chapter \ref{ModelingObjectContext.ch}, the problem definition, more motivational factors, applications and 
% challenges are presented.


Chapter\ref{3DModelofObjectContext.ch}, describes our challenges, method and details about the implementation. Chapter \ref{Evaluation.ch} talks about   experiments, results and their evaluation. 
%In Chapter\ref{UsingObjectContextforPlaceClassification.ch}, 
Finally, our model and descriptor applications in place classification are discussed, and an analytical study is done for the results expected for this application. Then the next potential steps that can be worked on in pursuit of this work is suggested in Chapter \ref{Conclusion.ch}.

%-------------------------------------------------------------------------------------------------------------------------------------------
\section{Problem Formulation}
\label{Formulation.sec}

In this work, our goal is to propose and develop a method to detect the ``3D context of objects'', rather than the object itself. The detected context can then be used as a very useful prior for object detection tasks and place categorization. The task of object detection can be formulated, in general, as a probability of observing some features in a scene $V$, given the presence of an object $O$:

\begin{equation}
 \label{ObjectDetction.eq}
    p(O|V)
\end{equation}

Features can be seen as two different sets, one directly related to the object (intrinsic features, such as size, shape, color,etc) and one related to its context. We call the former $V_I$ and the latter as $V_C$.

\begin{equation}
 \label{FeaturesSet.eq}
    V = {V_I,V_C}
\end{equation}

\begin{equation}
 \label{ObjectDetctionExtended.eq}
    P(O|V) = P(O|V_I,V_C)
\end{equation}

Using Bayes rules, we will have:

\begin{equation}
 \label{ObjectDetctionBayesapplied.eq}
    P(O|V) = \frac{P(V_I|O,V_C)}{P(V_I|V_C)} P(O|V_C)
\end{equation}

Our focus is on the second part ($P(O|V_C)$)


\section {Related Work}
\label{RelatedWork.sec}
     In this Section, we review other works that focused on using contextual information for object detection/recognition 
     or place classification.
     
%\subsection*{Object Context}
The usefulness of contextual information for object detection and recognition is discussed and
considered in \cite{TrollbaContexBased}, \cite{TorralbaContextualPriming}, \cite{PerkoLeonardisContextDriven}
and \cite{aydemir2012_3Dcontext}.

Torralba et al. in \cite{TrollbaContexBased}, used a low dimensional global image 
     representation for recognition of places and showed how contextual information can provide priors for object recognition. ``We show how we can use the context to predict properties of objects without even looking at the local visual evidence''. They consider the place category as a piece of contextual information and suggested that knowing the category of a place helps to find objects relevant to that place category. Once the place category (context) is determined, the system selects which object detectors should run. Their system is trained and tested on indoor and outdoor scenes using 2D images as inputs. Their system appears to perform well in known places but not as good for new places(unseen).
%      In following chapters we see how we defined the context of an object and We have a more fine-grained definition for the context of the object which is limited to a 
%      vicinity around the object.
%      In this work, we are suggesting an opposite direction which is using local object context information can help us in recognizing the category of a place.
%      
     
     
     {\it Torralba} in In \cite{TorralbaContextualPriming}, is considering a general representation of a whole scene in which an object is present, as its context (similar to the above). Context with their definition is proportional to the object's size with respect to each scene (image). So if the object is taking a small percentage of the image, the context will be mainly its background. In contrast, in a focused image where most pixels belong to the object, contextual information becomes intrinsic image features. Then they compute a context-centered object likelihood by modeling the relationship between contextual features and object properties.As a result, regions of an image with a higher probability of containing the object of interest get marked.   
     
     Perko and Leonardis in In \cite{PerkoLeonardisContextDriven} also proposed a method to focus attention on regions in an image (2D) where an object of interest is more likely to be found. They showed the performance of their approach for pedestrian detection in urban scenes. However, the method is said to apply to any object category and independent from task-specific models. They concluded that context-awareness provides benefits over pure local appearance-based processing and decreases search complexity while improving the robustness of object detection. 
     
     Compared to \textit{Torallba et al.} they used different features and descriptors and evaluated combining their contextual priming method with state-of-the-art object detectors. They used an additional contextual cue, the viewpoint prior, in addition to geometrical and texture features to estimate the position of the horizon.Geometrical features capture probability of each context point to belong to each of three classes of \textit{Ground}, \textit{Sky} or \textit{vertical} (Buildings, trees, ...). 
     
     For extracting contextual features, they consider a center for their object of interest then sample the features at 60 points around that object center (5 radii and 12 orientations). The obtained information is concentrated into one feature vector for that object center. They use linear SVM to train their model then convert the results into probabilities of a pedestrian being present at a given pixel position. 
     
     Their data acquisition setup, especially for estimating the viewpoint prior, needs to be adjusted based explicitly on the object category and application. 
     
     
     \textit{Aydemir et al.} in  \cite{aydemir2012_3Dcontext} argued that ``the placement of everyday objects is highly correlated to the 3D structure of an environment- as opposed to being correlated to the appearance of the environment''. So they believe in the existence of a correlation between an object and its natural context due to the physical support and reachability that the context can structurally offer. 
     
     They also train models for object context to focus the attention of object detectors to reduced search regions that match the learned models. They benefit from the recently available RGB-D sensors to obtain 3D data and show how the 3D context of objects can be a ``Strong indicator of object placement in everyday scenes''. They collected a large dataset from five different locations (but all university indoor rooms and corridors) using Microsoft Kinect. They are modeling object context using geometrical features of a neighborhood of annotated objects and trained a SVM, using weighted feature responses collected from different neighborhood parts, for capturing the geometry of they calculated histograms of surface normals on the 3D data corresponding to each selected region from an image.
     
     \textit{Ayedemit et al.}'s definition of context is close to the definition we used in our work. However, they used 2d images, plus depth information to model their context and do their evaluations, while we used full 3D reconstructed pointclouds of the scenes. Using the pointcloud allowed us to collect features from the observed environment without the need to focus on objects and their neighborhood (Training and test sets are full Pointlclouds and not pruned and adjusted scenes. Similar to us, they noticed how much height can be an informative factor for most object classes. 
     
     In our approach, we defined a point of view for our feature collection, which can mitigate the issue mentioned in \cite{aydemir2012_3Dcontext}, for larger objects. Our viewpoint provides a framework for the features to be collected from different points of view with respect to the object and makes it indifferent to the size or distance from the observer (camera). In \cite{TorralbaContextualPriming}, \cite{TrollbaContexBased} and \cite{PerkoLeonardisContextDriven}, objects that takes a small fraction of an image pixels can get neglected, while in \cite{aydemir2012_3Dcontext}, the framework performs less effective for larger objects, due to the variance of observable background as a result of difference in the viewpoint. 
     
     %In these works, the definition of object context becomes closer to what we consider.
     
% 2D features extracted from images are the information used in \cite{TrollbaContexBased}, 
% \cite{TorralbaContextualPriming} and \cite{PerkoLeonardisContextDriven} while in \cite{aydemir2012_3Dcontext}
% 3D information extracted from RGB-D frames builds models of 3D object context.
% The latter also analyzed the improvement in the performance of available object detectors using context information.
      
%       we use 3D information and features from full pointclouds of a place, not just single frames 
%       which helps capture spatial relations in the whole captured space.
%       It is useful, especially in place representation.
      
%     In \cite{5651280} Rusu et al. proposed Viewpoint Feature Histogram that encodes 3D geometry and pose. 
%     They suggested that this descriptor can be used for object recognition and pose estimation with high reliability.
%     In this work, we employ a feature descriptor, which contains their descriptor, for modeling the context of objects.
%     By the feature extraction method, we are proposing in this work, their descriptor is being used in a specific and novel way.
%     In contrast to its suggested usage to be extracted as a global descriptor for a pointcloud, based on a fixed viewpoint,
%     we are extracting it for several small subsets of a pointcloud and from several distinct viewpoints.
%     It is discussed in detail in Section \ref{Implementation.sec}.
     
% \subsection*{Place classification}
% 
%     In \cite{Vasudevan2008522}, Vasudevan and Siegwart proposed a representation for space based on objects. 
%     The authors discuss several algorithms, some of which only uses object category presence and some are more 
%     sophisticated using both objects and their relationships and also spatial structure of places.
%     
%     More researches are discussed later in Chapter \ref{UsingObjectContextforPlaceClassification.ch} which is 
%     specifically devoted to this particular application.
     
     
%----------------------------------------------------------------------------
\section {Contributions}
\label{Contributions.sec}
%----------------------------------------------------------------------------
In this work, we use 3D pointclouds of different place categories and annotated objects in them as input to train 3D models of object context. Then these models are applied on test pointclouds to determine which parts of the pointcloud (i.e., the place) are the most probable context for an object of interest. The object context is defined as surfaces in the vicinity of the object of interest. This includes surfaces the object is located on or next to and other objects that can usually be found beside the object of interest. 
For instance, if the object of interest is a \textit{kitchen water tap}, its context can be the 
\textit{kitchen sink} and the wall behind it. (Figure ~\ref{contextExample.figure})

\begin{figure}[t]
  \centerpsw{contextExample}{0.65\columnwidth}
  \caption[Illustration of a sample Context.]
  {A water tap as the object of interest and what we call its context. The context is surfaces within the green 
  bounding box.}
  \label{contextExample.figure}
\end{figure}

We propose a unique method and features to build 3D models of object context, which not only can be used as a prior to 
object detectors, but also as instrumental knowledge in building a 3D representation of place categories. 
By benefiting from our context models, the search area for object detection tasks can be reduced to parts of the scene with high 
likelihood of being the context for the object of interest. 
It also significantly increases the number of true positives in prediction results. 
Combining the information encoded in this model with other features and structural information of different place categories can 
result in a representation for places.
 
 To build Object Context models, we extracted features from several different regions of pointclouds with annotated objects as the training dataset. Then applied supervised learning methods to train the models. Pointclouds that we used are full 3D representations of a place such as an office, a kitchen, etc. 
 The learning is done on features which are representing local geometry and spatial location of contexts of objects.
 
 The most important points about this work can be briefly mentioned as follows:
 
 \begin{itemize}
  \item Emphasizing the role of context in object detection and its related applications.
  \item The Unique feature descriptor and methodology suggested to extract features and model the 3D context of objects.
  \item A simple method for annotating objects in pointclouds to be used as training data is proposed and implemented.
  \item The input data used in this work are full pointclouds of places rather than single frames with depth information or 
  2D images. The benefit of using 3D pointclouds over other data types is emphasized and shown.
  \item Unique method to evaluate the context models
  \item Suggesting the application of the Context model in place classification and proposing a method to do it.
  
 \end{itemize}

 %-----------------------------------------------------------------------------------------------------------------------
 %From here is moved from chapter 2 on April 21, 2021
 
%  \section{Problem Statement}
% \label{ProblemStatement.sec}
% here we need to define context of objects
% learn the model
% As discussed in the previous Chapter, in order to facilitate object detection and provide a means to classify places based on key objects 
% for a place category, in this thesis we proposed a structure and a method to build models for object context. 
% In this thesis we present an idea and a model to represent 3D context of object. As discussed in previous chapter, This model is 
% a very useful tool for both object detection and place categorization in which object information is used.
% 
% Based on the definition stated in Chapter \ref{Introduction.ch}, by context of the object we are referring to the surfaces 
% in a neighborhood around it(Figure \ref{contextExample.figure}). 
% The problem that is addressed in this research, is how to define a suitable feature descriptor to represent those surfaces 
% as the context of the object class. 
% The trained model would be applied on pointclouds of new scenes to determine locations in the scene that are likely
% to contain the object (regardless of an object being actually present in thoes locations).
% The expected result, is a Likelihood map for different regions of a scene (pointcloud) that corresponds to the presense of an object class.

\section{Motivation and Applications}
\label{MotivationandApplications.sec}
A motive for this work is to achieve a complete and robust 3D descriptor for places based on object contexts 
rather than objects. 
When a human agent enters a room, intuitively, he can decide about the category of the place based on a simple and 
shallow perception, with reasonable confidence. 
Even if the room is not furnished yet and there are not many objects in it, to help the agent in determining its 
category. 
Perhaps more than the presence of some objects in a place, it is the possibility of their presence that helps us 
determine the place's category. Geometrical and spatial information about different regions in a place can help us imagine what object categories can be placed in it. In other words, what sort of object context can be found in that place. 
For sure other information adds up to this knowledge to make a human able to distinguish between places, such as general 
shape and size of the place also its location regarding other places. 
A model for an object context can provide us with the likelihood of object presence.
%Although, the Context Model, which we are talking about in this work, is not enough for making this decision, but is a useful 
%prior.

Among the applications that are considerable for the model, the most obvious ones are as follows:


\subsubsection*{Object Detection}

Object detection is an expensive and complex task. Searching for the context of 
an object is, most of the time, significantly easier than the object itself, especially when the searched object is on a relatively small scale comparing to the search area. 
Once the context is detected, the search for the actual object is done in a reduced area, within the parts predicted as the context.  
In addition, false positives are avoided or at least reduced. For example, when a table is detected as the context for a telephone, any similar shape  on the floor will not be considered by the object detector, which otherwise would have detected it as a candidate. 

\subsubsection*{Place Classification}
 
A critical piece of information that can help a robotic system to be able to distinguish between different place categories 
is knowing about the objects which are expected to be found in a specific category of a place. 

If the robot is looking for a kitchen, it helps if it knows about some objects that are most probable to be found in it and 
perhaps not in other places. 
For instance, a kitchen sink is such a discriminative object. 
If the robot can find such an object in a place, it can obtain reasonable confidence about that place to be a kitchen. 

{\it Viswanathan et al.} in \cite{P.Viswanathan}, also pointed to the fact that object detection is an excellent prior for place 
categorization. {\it T. Southey and J. J. Little} in \cite{southey2006object} argued that {\it rooms} are defined by their geometric 
properties, while the definition of {\it places} is based on objects they contain, and activities take place in them.  

On the other hand, the place classification system would be more robust if it can recognize the place, using object information, 
even when the actual object is not present in the place, but the presence is expected.
Here is where the contextual model of the object finds its importance. When the context is present, there is no need to detect the actual 
object to classify the place.

 
 
 \subsubsection*{Object Placement} 
 
 An intuitive application for the model of object context is finding a suitable location or surface to put an object.
 When a robot knows the context model of an object, it can place it correctly by finding matches of that model in its 
 surrounding.
 For instance, a robot that performs typical household chores, like cleaning or serving guests, can benefit from it.  
 
 %-----------------------------------------------------------------------------------------------------------------------
 %Until Here is moved from chapter 2 on April 21, 2021


