%------------------------------------------------------------------------
\chapter{Evaluation}
\label{Evaluation.ch}
% ------------------------------------------------------------------------
% revised 21-02-13
% \section{An example Scenario}
% \label{Scenario.sec}
% Point-clouds get captured
% Apply the models on the pointclouds
% By matches for models that are found, we know what object classes should be expected in that place
% Based on this result and the ground truth for each place category, the probability of belonging the place to each category is estimated.
% 
% In this scenario, a situation is considered where a mobile robot wants to look around in a building and make a map including 
% semantic information about the places that it meets in the map. 
% The robot builds a 3D map of the building using, for instance, a SLAM application. 
% Employing the context models, which has been trained for different key object classes, the robot can estimate which object 
% classes are probable to be found in each place that is present in the map. 
% 
% By key objects we are referring to distinctive objects for a place category. These specific object classes should be chosen based on 
% a ground truth which is defined in advance for a place category. It is discussed in Chapter ~\ref{UsingObjectContextforPlaceClassification.ch}
% in more detail.
% 
% Then using the results, showing which object classes are expected in a place and based on the ground truth, for key objects of each place
% category, the place is classified. 
% 
% But for the building of the model it self. the same steps mentioned above for capturing the point cloud would be performed,
% on the pre processed captured point cloud of a place using the annotation tool we have developed, key objects for that 
% specific place category would be annotated. Feature extraction would be run on this annotated point cloud giving training 
% samples. Train samples would be combined from several different pointclouds for the same place category but different 
% instances, Resulting data set would be divided into train, validation and test sets. Using SVM with two kernels the model would be 
% trained.
% 
% More details would be presented in section ~\ref{Implementation.sec}
% 
% In this chapter, we define our proposed methoed and the the fueatures used to model the Object Contexts. Also details about the implementation, obtaining train and test datasets, Data anotation and expected results. But first lets take a look at some of the chanllenges we had.
% 
% Moved from Chapter 2 on April 21, 2021
% \section{Challenges}
% \label{Challenges.sec}
% 
% There are challenges regarding modeling the object context that follows:
% 
% \begin{itemize}
% 
%  \item Train set quality: There could be situations and scenes which are completely different from
% 	what have been considered in train set to build the context model. The closer to an all encompassing dataset we can get, the more accurate model we can have. This would be a chanllenge in most projects that involve learning.
% 	
%  \item Annotation of objects in Training data: An easy and reliable tool was needed to annotate 3D objects in pointclouds.
% 	
%  \item Pointcloud quality: The quality of pointclouds is a critical issue. The amount of smoothness in the pointcloud and missing points have a 
%  significant effect on the extracted features, which are used to describe surfaces, and as a result, on the context model. 
%  When pointclouds are being captured, if due to the viewpoints of the sensor some parts of a surface are occluded, 
%  the resulting pointcloud might lack important points.
%  The geometry that is extracted from such surfaces is not accurate and in some cases are wrong.
%  
%  \item Significant difference in the number of positive samples (feature vectors extracted from a region with annotated object presence) against negative ones, in feature vectors extracted form training pointclouds.
%  
% \end{itemize}
% 
% 
% \section{2D vs 3D context, benefits of 3D}
% Popularity of devices such as Microsoft Kinect, has made 3D data widely accessible. 
% Therefore, many researchers and academics have shown great desire to use 3D information in their researches, 
% which gives a lot more information about our surrounding compared to 2D data. 
% Using 3D data has made it simple to capture geometrical information in addition to visual properties. 
% From the output of this type of sensors, we can easily and directly have access to depth information and 3D 
% coordinates of any points that are perceived.
% 
% Geometrical features, which are obtained from 3D information, enhances performance of systems significantly in tasks like feature 
% based object detection compared to relying only on visual information. 
% Particularly, in the problem we are dealing with, to build a model for object context, depending on features which only encode 
% changes in intensity and color is not enough. The geometry of surfaces needed to be analyzed and 3D information facilitates and improves this analysis.
% 
% 
% \section{Methodology}
% \label{Implementation.sec}
% 
% In this section, the procedure of building the context model and how it is applied, is described in detail. 
% The input in this procedure is the pointcloud of a scene and the output is a list of probability values assigned to all regions in
% the scene. 
% The probability value shows if the region is likely to be a context. 
% The input cloud is first preprocessed and then get annotated with instances of the object class whose context model is being built. 
% Feature extraction would run on this annotated pointcloud and result in the training samples. 
% Train samples would be combined from several different pointclouds for the same object class to make the train set. 
% Then the train set is given to SVM classifier to make the context model. A block diagram in figure ~\ref{SystemOverview.figure} shows 
% the system overview.
% 
% \begin{figure}[t]
%   \centerpsw{SystemOverview}{0.8\columnwidth}
%   \caption[System Overview]
%   {The overview of the system. Box on the left side shows the Train part of the system while the one on the right shows the prediction part.}
%   \label{SystemOverview.figure}
% \end{figure}
% 
% In the following subsection, we introduce the features used in this work and description of the modules of the system 
% will come next.
% 
% Features used
% We used PCL ~\cite{Rusu_ICRA2011_PCL} for pointcloud processing and Microsoft Kinect with OpenNi driver to capture these pointclouds.
% In this data type 3D coordinates and color information of a point is saved in a record and then an array of these records forms 
% the pointcloud.
% 
% \subsection{Object context descriptor}
% \label{OCD.ssec}
% 
% The feature descriptor we proposed in this work, consists of 2 parts which are discussed below.
% 
% The first part includes two fields :
% a field that captures a rough estimation of query surface orientation with respect to the vector of gravity.
% This feature is computed by estimating the angle between average normal of the surface and the gravity vector.
% We consider a unit vector along the z axis in the world coordinate system as the gravity vector.
% It is noticeable that the position and orientation of the camera with respect to this coordinate system is known and used as
% the ground truth.
% 
% Second field is the average height of the query surface from which the features are extracted with respect to the 
% floor.
% The floor is assumed to have the lowest height or vertical position in the pointcloud.
%  
% And the second part of the feature vector is a histogram which captures the geometry of the blob with respect to a specific view point.
% This histogram is called VFH (~\cite{5651280}).
% 
% \subsubsection*{The Viewpoint Feature Histogram (VFH)}
% \label{VFH.ssec}
%  
% VFH consists of two components (~\cite{VFH_Definition} and ~\cite{5651280}):
% 
% \begin{itemize}
%  \item A histogram for viewpoints(128 bins)
%  \item Extended Fast Point feature histogram(FPFH:4*45 bins)
% \end{itemize}
% 
% The viewpoint component is computed as a histogram of the angles that direction of the viewpoint  makes with each surface
% normals (Figure \ref{VFH_ViewPoint_component.figure}). 
% It is important to notice that, this angle is between the central direction of the viewpoint and the normal, not the view angle of
% the normal which would have made it not scale invariant. 
% 
% \begin{figure}[t]
%   \centerpsw{ViewPoint_component}{0.8\columnwidth}
%   \caption[ViewPoint Component of VFH]
%   {Viewpoint part of the feature vector.\cite{VFH_Definition}}
%   \label{VFH_ViewPoint_component.figure}
% \end{figure}
% 
% The second component measures the relative pan, tilt and yaw angles that are measured between direction of 
% the viewpoint at the central point and each of the normals on the surface. 
% So it consists of accumulated FPFH values for points in the query pointcloud.
% That is the reason why it is referred here \ref{VFH_plot.figure} as extended FPFH.
% 
% FPFH or Fast Point Feature Histogram itself can be considered as a revised version of PFH. FPFH reduces the complexity 
% of PFH algorithm from $ O(n k^{2}) $ to $ O(n k) $ where $n$ is the number of points in the query pointcloud and $k$ is 
% the number of points in the considered neighborhood of each query point. 
% This is the reason why it is called Fast PFH.
% Although it reduces the informativeness of the feature but it would be still enough powerful and impressively faster 
% to compute. 
% 
% PFH captures 3D geometry of the surfaces around a query point and it is invariant to the 6D pose of the query blob's 
% surface. 
% VFH is robust to different sampling densities or noise levels in the neighborhood of the query point. 
% The descriptor is computed by estimating a histogram with concatenated 45 bins for four different elements.
% measured for every possible pairs of points in the query point's neighborhood ($ O(n k^{2}) $). 
% These elements consist of three angles and a distance. 
% In FPFH \cite{FPFH_Definition}, instead of encoding this information for all pairs of points in a neighborhood, It will only 
% take pairs between the query point and its neighbors ($ O(n k) $). 
% At this stage it is referred to as simplified PFH or SPFH. 
% Then to keep the amount of information that this feature can provide, it also estimates SPFH for all $k$ points in the neighborhood.
% The resulting values are integrated with a normalization to result in the FPFH of the query point(equation ~\ref{FPFH.eq}).
% \begin{equation}
%  \label{FPFH.eq}
%  FPFH(P_{q}) = SPFH(P_{q}) + \frac{1}{k} \sum_{i=1}^k {\frac{1}{\omega_k}} \cdot SPFH(k)
% \end{equation}
% Figure  ~\ref{FPFH.figure} shows the encoded elements and their relation in FPFH.
% 
% \begin{figure}[t]
%   \centerpsw{fpfh_component}{0.65\columnwidth}
%   \caption[FPFH elements.]
%   {Three angles and a distance encoded in FPFH and the points considered.\cite{VFH_Definition}}
%   \label{FPFH.figure}
% \end{figure}
% 
% In VFH the resulting FPFH of points in the pointcloud are integrated to produce one of the two 
% components(extended FPFH).
% The histogram in this component has 180 bins (4*45 bin).
% By adding the view point component (128 bin), it produces a 308 bin histogram. 
% Unlike FPFH, as a result of adding the viewpoint component VFH is dependent to viewpoint which is the reason we chose
% it as a part of our descriptor.
% The viewpoint component is the element that makes distinction between features captured from a single query blob viewed 
% from different directions.
% It is discussed in more detail in Section \ref{FeatureExtract.ssec}.
% In spite of its dependency to a viewpoint it still preserves the property of being scale invariant. 
% Figure ~\ref{VFH_plot.figure} shows a sample plot of VFH and its components.
% 
% \begin{figure}[t]
%   \centerpsw{vfh_histogram}{0.65\columnwidth}
%   \caption[VFH histogram]
%   {A sample plot of VFH and its components.\cite{VFH_Definition}}
%   \label{VFH_plot.figure}
% \end{figure}
% 
% The images and definitions are taken from Point Cloud official website.
% 
% The resulting object context descriptor is a vector with 310 elements that encodes a general orientation, height and
% geometrical properties of the query blob.
% The structure of this descriptor and its elements are shown in Table ~\ref{Descriptor.table}.
% 
% \begin{table}
% \centering
% \caption
% [Object context descriptor structure.]
% {Structure of object context descriptor.}
% \label{Descriptor.table}
% \begin{tabular}{|c|c|c|c|}
% \hline
% Field number & 1 & 2 & 3-310 \\
% \hline
%       Field content & Orientation & Height & VFH \\
% \hline
% \end{tabular}
% \end{table}
% 
% Libraries
% \subsection*{Used Libraries in Implementation}
% 
% Most of the implementation is done in C++. 
% There are some scripts for evaluation part of the results and creating the dataset for experiments written in MATLAB and also 
% scripts creating the experiments and the experimental environment are in python. 
% Libraries used in this work are as follow:
% 
% \begin{itemize}
%  \item PCL \cite{Rusu_ICRA2011_PCL}
%  \item Eigen \cite{eigenweb}
%  \item LIBSVM \cite{LIBSVM}
% \end{itemize}
% 
% 
% \subsection{System Modules}
% As illustrated in Figure ~\ref{SystemOverview.figure}, apart from SVM (\cite{LIBSVM} and 
% \cite{li2010holistic}), there are three modules in the system:
% 
% \begin{itemize}
%   \item PreProcess
%   \item Annotation
%   \item FeatureExtract
% \end{itemize}
% 
% \subsubsection{PreProcess}
% \label{PreProcess.ssec}
%  In this module, the input pointclouds get prepared for annotation and feature extraction. 
%  The pointclouds we used in this work are saved in "PCD" format and their point type is PointXYZRGB which is PCL point type. 
%  The processes for capturing these pointclouds is discussed in Section ~\ref{ExperimentalSetup.sec}. 
%  First step in preparation of the input is transforming it.
%  Based on the location and orientation of the sensor in time $t_0$ the resulting point cloud is transformed from 
%  sensor coordinate system to the world's coordinate system.
%  The sensor is in a fixed orientation and vertical position in time the first frame is captured for all pointclouds 
%  used in experiments of this work.
%  
%  As a different PCL point type, in which labels can be saved for each point, is used in the rest of the process, 
%  in this step a conversion of the transformed pointcloud into that type  is also carried out.
%  
%  This new point type is PointXYZRGBL which includes a field in each point for label which is used later in annotation 
% tool and also in feature extract to decide about the class of the sample in train data set. 
%  The most important step, which is taken here is estimation of surface normals which is essential for feature extraction. 
%  The accuracy of the VFH feature is directly dependent to the accuracy of the normals. 
%  At first we were doing this process in feature extract module and for each query blob at the moment which added a huge overhead to
%  the process while it could be done independently and once for each point cloud. 
%  Here the normals for the whole pointcloud are estimated once and saved in a separate pcd file to be used in 
%  feature extract. 
%  It uses NormalEstimation class in PCL.
%  
%  There is also one more product for this module, which is discussed more in ~\ref{FeatureExtract.ssec}.
%  It is a generated cube-shaped pointcloud with length, width and height of the input cloud, filled with points.
%  The density of these points is determined by a fixed radial distance between them.
%  It is used as a source for picking viewpoints that are needed in feature extraction.
%  
%  Therefore, the results of this module are as follows:
%  \begin{itemize}
%   \item Point-cloud's normals
%   \item The transformed version of the input cloud
%   \item Converted version
%   \item TFP-cloud
%  \end{itemize}
%  
% \begin{algorithm}[t]
% \begin{algorithmic}[1]
% \REQUIRE Input Point-cloud(XYZRGB).
% \REQUIRE Sensor Location and orientation.
% \medskip
% 
% \STATE Transfer the input cloud based on Sensor location and orientation from camera coordinate to world coordinate, and store the 
% result.
% \STATE Estimate Normals and store.
% \STATE Convert the Pointcloud\_Transfered to Point type XYZRGBL and store in Pointcloud\_Converted.
% \STATE Estimate 3DMINMAX of the Pointcloud\_Converted.
% \STATE Using result from previous step, generate TFP pointcloud.
% 
% \medskip
% \ENSURE Pointcloud\_Transfered.
% \ENSURE Pointcloud\_Normals.
% \ENSURE Pointcloud\_Converted(XYZRGBL).
% \ENSURE Pointcloud\_Generated(TFP).
% 
% \end{algorithmic}
% \caption[PreProcess.]
% {A brief algorithmic description of PreProcess.}
% \label{Preprocess.algorithm}
% \end{algorithm}
% 
% 
% 
%  
%  
% \subsubsection{Annotation}
% \label{Annotation.ssec}
% In this module the transformed and converted version of the pointcloud is loaded to get annotated. 
% The objects needed to be annotated in the scene, are selected by clicking roughly on their center. 
% The labels of the points belonging to them are assigned with a value representing object points. 
% Objects from different classes could be labeled with different values (\ref{Annotation.algorithm}).
% 
% \begin{algorithm}[t]
% \begin{algorithmic}[1]
% \REQUIRE Pointcloud\_Transfered(XYZRGB).
% \REQUIRE Object radius(rough estimate).
% \medskip
% 
% \STATE Visualize input cloud.
% \FORALL{objects to be annotated}
%   \STATE Run Point picking procedure.
%   \STATE Extract neighbor points indexes with respect to the input object radius.
%   \STATE Label points whose indexes are extracted in previous step.
% \ENDFOR
% \STATE Store the pointcloud with the labels in output cloud.
% 
% \medskip
% \ENSURE Pointcloud\_Annotated(XYZRGBL).
% \end{algorithmic}
% \caption[Annotation.]
% {A brief algorithmic description of Annotation.}
% \label{Annotation.algorithm}
% \end{algorithm}
% 
% 
% Figure ~\ref{TrashbinBounding.figure}, shows a scene in an office at Royal Institute of Technology(KTH). 
% The object of interest here is the trash bin, which is marked by a green bounding box. 
% This figure is an image of the scene whose pointcloud is available in our dataset and the annotation is done on 
% the pointcloud. 
% In Figure ~\ref{Annotation.figure} the annotated object could be seen in red within the scene's pointcloud.
% It can be seen that some part of the floor is also colored in red, which means that part of the floor is also 
% annotated as the object.
% The reason is the object radius, that is given to annotation tool was not accurate enough or the center of the 
% object was not picked accurately.
% But it does not harm the result and this much of accuracy is more than enough.
% Usually annotation is done using a bounding box which consider the box surrounding the object as the object.
% Here, we exactly separate the points of the object by benefiting from the 3D coordinates of the points in the 
% pointcloud.
% As it is described in the algorithm ~\ref{Annotation.algorithm}, object segmentation is done automatically in a very 
% simple way.
% Based on the picked object center, selecting points from the 3D neighborhood of it separates object points from the 
% rest of the pointclouds.
% 
% \begin{figure}[t]
%   \centerpsw{TrashbinBounding}{0.65\columnwidth}
%   \caption[Example scene and object for Annotation tool]
%   {An example scene and a trash bin marked by a bounding box as the object which is being annotated (Best viewed in color).}
%   \label{TrashbinBounding.figure}
% \end{figure}
% 
% \begin{figure}[t]
%   \centerpsw{Annotation}{0.65\columnwidth}
%   \caption[Annotation tool result]
%   {An example of annotation result on the pointcloud, red points assumed to belong to the object(Best viewed in color).}
%   \label{Annotation.figure}
% \end{figure}
% 
% This package is available with source to be used by other researchers and get improved by developers.\cite{AnnotationGithub}
% 
% Feature extraction
% 
% \subsubsection{FeatureExtract}
% \label{FeatureExtract.ssec}
% First, we Introduce some terms used in this part. They are as follows:
% \begin{itemize}
%   \item Blob: A sub set of the Point-cloud which includes a number of points in a neighborhood within a specific radius.
%   \item Query Blob: The blob from which the features are being extracted.
%   \item QPoint: The Query Point that is the center of the query blob. It is a point on the context of the object.
%   \item OPoint: The Object point is the point in surrounding of the Qpoint which is an object point in a positive sample.
%  \end{itemize}
%  
%  Instances of these elements are illustrated in Figure ~\ref{FEStructure.figure} for the example scene that we saw in Figure 
%  ~\ref{TrashbinBounding.figure}. 
%  The red point in the figure shows the $Query$ $Point$, which is a point picked from the context of the object and the features are extracted from the sphere surrounding it. The sphere is also visible in the figure which is the 
% $Query$ $blob$. 
%  The yellow point is the $OPoint$ or object point which is a point on the annotated object. 
%  
%  \begin{figure}[t]
%   \centerpsw{FEStructure}{0.65\columnwidth}
%   \caption[Illustration of the items used in Feature Extract.]
%   {Structure used in Feature extraction; The red point is the QPoint; The sphere surrounding it, is the Query Blob; The yellow
%   point is the OPoint (Best viewed in color).}
%   \label{FEStructure.figure}
% \end{figure}
%  
% In this module, the converted and transformed pointcloud and its normals are loaded.
% To build the data set for train or test, Features are extracted for a number of keypoints from all over 
% the pointcloud.
% Keypoint selection is described later in this section.
% 
% The goal here is to get samples which are useful in building the context model for objects of interest. 
% This is done in a way that for each key point, which is considered as a QPoint when it is selected for feature extract, 
% we take a neighborhood of points and their corresponding point-normals to extract features from (Query Blob). 
% So thing that happens here is studding the geometry around the query point as a candidate geometry of a positive context.
% 
% The important idea applied here is the view point used in VFH.
% For each QPoint and its corresponding Query blob, a number of points in a radius from the QPoint is considered as the view points 
% (OPoints).
% A sample is extracted for each pair of $Qpoint$ and $Opint_i$.
% This way due to the difference in viewpoint, the features extracted for a single query point and its query blob would be slightly 
% different. 
% This will result in discrimination between same context viewed from different view point which is to our benefit.
% 
% As mentioned above, these viewpoints are candidates for being object points.
% In each sample if the OPoint in a pair of $(QPoint,OPint)$, is actually an object point we want this sample to be labeled as 
% a positive sample and if not, to be a negative one. 
% This means that if the features are extracted from a query blob and for a view point which is within a specific distance from it,
% is located on the object of interest the query blob is actually a context for that object.
% The view point also helps to encode the most probable location of the object with respect to the positive context. 
% 
% Another important point here is that we want the model to be able to locate candidates for context in a test pointcloud where 
% object may be not present at the moment but its context is.
% Therefor, we need the OPoints to be independent from actual points in the pointcloud.
% 
% This is where TFP-cloud finds its role.
% This is a pointcloud generated in accordance to the input pointcloud which includes points with fixed structure to be used as 
% candidate OPoints in feature extract.
% Using these points as OPoints, we have fixed viewpoints to sample the context for. 
% To have the same structure in train and test data, we use the same OPoints in feature extract for train set as well.
% The only difference in feature extract for train and test is that in train it is checked if the OPoint is on the object, using 
% labels from annotated cloud. 
% Then, if the answer is positive, the extracted sample would be a positive one. 
% If not, it would be a negative sample. 
% But in feature extract for test, we just do not do this check.
% 
% QPoints would be selected in a loop on key points in the pointcloud. 
% The key points are all points in a down sampled version of the input cloud with a radius that is dependent to the 
% size of the object of interest. 
% This way, we have a uniformly distributed key points in the input pointcloud which will make the sampling more informative.
% For each of these QPoints, a blob around them is considered from the input cloud with point normals(Query blob).
% Therefore, Qpoints come from the down sampled cloud while query blob is from the original cloud.
% 
% In another loop, the OPoints are assigned  as the view point in feature extraction of each sample. 
% Feature vector would be extracted for this blob and the OPoint. 
% Figure ~\ref{PointParameters_Diagram.figure} shows these points and their related parameters. 
% The values mentioned in the figure is for an example object class.
% 
% \begin{figure}[t]
%   \centerpsw{PointParameters_Diagram}{0.65\columnwidth}
%   \caption[Feature extract structure]
%   {QPoint and generated OPoints and their spacial relations.}
%   \label{PointParameters_Diagram.figure}
% \end{figure}
% 
% \begin{algorithm}[t]
% \begin{algorithmic}[1]
% \REQUIRE Pointcloud\_annotated or Pointcloud\_converted(Depending train or test).
% \REQUIRE Pointcloud\_Normals.
% \REQUIRE Pointcloud\_Generated.
% \REQUIRE S\_Radius
% \REQUIRE OPoint\_Radius
% \medskip
% 
% \STATE Key-point selection.
% \FORALL{Key-points}
%   \STATE Extract Query blob.
%   \STATE Extract indexes of generated points within S\_Radius from the key-point.
%   \FORALL {extracted generated points}
%     \STATE Assign generated point to OPoint.
%     \STATE Extract features for the OPoint and the Query blob
%     \IF{Features are for train}
%       \IF{There is an object point within OPoint\_Radius of the OPoint}
% 	\STATE Label the sample as positive
%       \ELSE
% 	\STATE Label the sample as negative
%       \ENDIF
%      \ENDIF
%    \ENDFOR
%      \STATE Store the sample
% \ENDFOR
% 
% \medskip
% \ENSURE Pointcloud\_Extracted samples.
% \end{algorithmic}
% \caption[Feature Extract.]
% {A brief algorithmic description of Feature Extract.}
% \label{FeatureEXtract.algorithm}
% \end{algorithm}
% Learning
% 
% \subsubsection{Learning}
% \label{Learning.ssec}
% 
% learning is done using libsvm
% After Feature extraction, the resulting data set for train is made in a way that it includes samples from several pointclouds 
% for the same object class. 
% In first experiments we used a single Gaussian kernel for the whole feature vector whose fields are of two different types. 
% As mentioned before part one is two float numbers and part two is a histogram. 
% Later we separated these two parts to apply independent kernels on each. 
% 
% Another important issue here is the distribution of samples in each of positive and negative classes. 
% From the data we extracted, in average, about one percent of the samples were positive and the rest were negative.
% This is because the annotated object is a small part of the pointcloud and positive samples are the samples extracted from its 
% surrounding. 
% This significant difference in number of samples in positive and negative classes makes the classifier to prefer classifying test samples into 
% the class with more train data.
% 
% In addition, this issue has some other effects that make the classier confused. 
% The features extracted from blobs in object's neighborhood may be so similar to some feature extracted from a blob far from the 
% object, while the first one would be a positive sample in train data and the second one would be a negative sample. 
% For instance, features extracted for a cup context, which can be a table's surface, will get positive label when it is extracted 
% from a blob close to where the annotated cup is located. 
% But features from the same surface which is extracted from a region far from the annotated object gets a negative label.
% Because there was no annotation nearby to make it positive. 
% 
% In order to solve the mentioned issue and both of its effects, or at least improve the result toward our goal, we needed to do 
% an unbalanced weighting for our samples. 
% We decided to assign different weights for misclassification cost in SVM binary classifier. 
% It should be in a way that not only compensates the lower amount of positive samples, but also emphasizes the importance of 
% positive samples compared to negative ones. 
% It can be inferred that there should be a big weight for positive class and rather small value for negative class.
% In Section ~\ref{ExperimentalSetup.sec} a parameter selection procedure used to find suitable values for them is discussed.      
% 
% The way datasets and experimental environment is created is discussed in Section ~\ref{ExperimentalSetup.sec}.
% 
% 
% parameters
% \subsection{Parameters}
% \label{Parameters.ssec}
% 
% Feature extract parameters:
% As mentioned before and depicted in figure ~\ref{PointParameters_Diagram.figure} there are some parameters that play great role in
% achieving good results. 
% These parameters are:
% 
% \begin{itemize}
%  \item  B-Radius : Radius of the query blob (from which features are extracted) of points from the pointcloud with original
%  density.
%  \item Voxel-Radius : Radius used for voxel down-sampling. 
%  \item S-Radius = : Radius of the sphere to search object point in. 
%  It depends on the object class.
%  \item OPoint-Radius : This is the radius of the neighborhood of the generated point to look for actual Object point.
% \end{itemize}
% 
% These parameters directly or indirectly are dependent to the average size of the object class that we are making the context model
% for. 
% This dependency is not so tight, it means that the object size does not need to be very accurate. 
% As long as these values does not cause the object to be missed, they are acceptable.
% The object can be missed if the down sampling radius, which depends on the object size, is too large. 
% On the other hands, small values causes the complexity to increase and computation time to get too long.  
% As a result, we reduced the number of dependencies to a single parameter which a rough estimate of the object size.
% 
% 
% Learning parameters:
% \begin{itemize}
%  \item $w_i$: Weight for class labeled (i)
%  \item $c$: Sets the cost value for misclassification.
%  $w_i$ acts as a coefficient for $c$, so the combination of their values will set the cost value for each class.
%  \item $g$: Sets the value of gamma in Gaussian or chi-square kernels, which is clearly a very important factor for the result we can 
% expect from the classifier. 
%  \item kernel type
%  \item weight for kernels in multi kernel setup
% \end{itemize}
% 
% 
% \subsection{Qualitative study of samples}
% \label{QStudy.ssec}
% 
% It can help if we first check some plots showing how does some random samples look like and how discriminative the feature vector 
% can be.
% Figures ~\ref{CompareNegHis.figure} and ~\ref{ComparePosHist.figure} show the plot of two random negative and two random 
% positive samples.   
% Results
% \begin{figure}[htp]
%   \begin{center}
%     \subfigure[Compare two random negative samples.]{\label{CompareNegHis.figure}\includegraphics[width=2in,height=2in]{./Figures/CompareNegHis}}
%     \subfigure[Compare two random Positive samples.]{\label{ComparePosHist.figure}\includegraphics[width=2in,height=2in]{./Figures/ComparePosHist}}
%     \subfigure[Distance between random negative samples.]{\label{DistanceOFTwoNegHist.figure}\includegraphics[width=2in,height=2in]{./Figures/DistanceOFTwoNegHist}}
%     \subfigure[Distance between random Positive samples.]{\label{DistanceOFTwoPosHist.figure}\includegraphics[width=2in,height=2in]{./Figures/DistanceOFTwoPosHist}}
%   \end{center}
%   \caption[Comparative plots of random samples.]
%   {Plots of feature vectors of random positive and negative samples and the euclidean distances between them. This comparison shows the challenge in training a classifier for these samples.}
%   \label{ComparativeFVPlot.figure:edge}
% \end{figure}
% \begin{figure}
%   \centerpsw{CompareNegHis}{0.65\columnwidth}
%   \caption[Compare two random negative samples]
%   {Two random Negative samples plotted on top of each other to compare.}
%   \label{CompareNegHis.figure} 
% \end{figure}
% 
% \begin{figure}[t]
% \centerpsw{ComparePosHist}{0.65\columnwidth}
%   \caption[Compare two random Positive samples]
%   {Two random Positive samples plotted on top of each other to compare.}
%  \label{ComparePosHist.figure}
% \end{figure}
% 
% \begin{figure}[t]
%  \centerpsw{DistanceOFTwoNegHist}{0.65\columnwidth}
%   \caption[Distance between random negative samples]
%   {Distance between random negative samples.}
%  \label{DistanceOFTwoNegHist.figure}
% \end{figure}
% 
% \begin{figure}[t]
%  \centerpsw{DistanceOFTwoPosHist}{0.65\columnwidth}
%   \caption[Distance between random Positive samples]
%   {Distance between random Positive samples.}
%  \label{DistanceOFTwoPosHist.figure}
% \end{figure}
% 
% \begin{figure}[t]
%   \centerpsw{DistanceOFPosandNegHist}{0.65\columnwidth}
%   \caption[Distance of Positive and Negative samples]
%   {Distance between random positive and negative samples.}
%   \label{DistanceOFPosandNegHist.figure}
% \end{figure}
% 
% In order to have a more clear idea about differences between samples, we can also look at the distances between random samples
% in the same class and compare them to the distance of samples from different classes. 
% Figure ~\ref{DistanceOFTwoNegHist.figure} shows the distance in two random negative samples. 
% It should be noticed that the first part of the plot reflects the view point component of the feature vector.
%  
% 
% 
% And Figure ~\ref{DistanceOFTwoPosHist.figure} is the same plot for positive samples. 
% Features extracted for each class can have big differences as well, which was predictable. 
% Considering different surfaces captured as context by these features that can belong to a class causes the differences. 
% A positive sample can be representing, for instance, the surface of a table or a wall or the meeting region of these 
% two surfaces. 
% This comparison gives us an idea about how challenging it is to train a classifier for such data.
% As mentioned before, it is also important that more attention to be put on learning the positive samples rather than 
% negative ones. 
% 
% 
% 
% Figure ~\ref{DistanceOFPosandNegHist.figure} is showing the distance between random positive and negative samples.


\section{Experimental Setup}
\label{ExperimentalSetup.sec}
% Environment setup
In order to do an evaluation on our method and the model, some experiments are carried out.
To prepare a data-set for our experiments we needed to capture several pointclouds from different scenes and places which 
include different object classes.


To capture pointclouds we used different tools:
{\it RGBDSLAM} \cite{RGBDSLAM} is an open source  package available in ROS. 
Using this package and kinect sensor with OpenNi driver a 3D model of a scene can be captured. 
The results are saved as pcd files.
Pointclouds were captured from different types of places from KTH campus like offices, Kitchens and bathrooms including 
different types of object that can be found in those places.

There is also a project in CVAP\footnote{Computer Vision and Active Perception lab.} at KTH called KINECT@HOME \cite{aydemir2012kinect} which is a web based application uses
kinect out puts to build 3D mesh model of objects and places. 
A very good property of this system is that people from any part of the world capture their own video with kinect from different 
places and scenes and post the videos to a server where the 3D reconstruction happens. 
Through this means a good dataset can be prepared to train and test models.
Of course, not all of the resulting pointclouds from this database are useful for our purpose due to the content and
quality, but still there are applicable ones.
%but there were a lot of models that we chose among and 
%converted them to the pointcloud type that was compatible with our system.

%TODO: Silberman's data set.

% Data set
All resulting pointclouds are saved and based on their content, a name is assigned to them. 
Then, they are divided into three subsets of train, validation and test. 
Although, samples from the same pointcloud could be divided into train and validation sets, but we preferred to make them 
separate even from pointcloud level to be sure of having reliable results. 
Figure \ref{TrainClouds1.figure:edge} shows full pointclouds with different type of trash bins annotated in them. 
These are the pointclouds used in experiments.
%Each of these train and validation sets included different category of places.
% Procedure


% \begin{figure} [htp]
%      \begin{center}
%     \subfigure[A partial view of a bathroom.]{\label{TrainClouds1.figure:Bathroom}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Bathroom_6thfloor}}
%     \subfigure[Full pointcloud of a kitchen.]{\label{TrainClouds1.figure:Kitchen4th}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Kitchen_4thFloor}} \\
%     \subfigure[A partial view of a office.]{\label{TrainClouds1.figure:Office607}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Office_607_Desk}}
%     \subfigure[A full pointcloud of an office.]{\label{TrainClouds1.figure:Office518}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Office_518}} \\
%     \subfigure[A full pointcloud of another kitchen in the same building.]{\label{TrainClouds1.figure:Kitchen6th}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Kitchen_6thFloor}}
%      \subfigure[A partial view of an office with lots of missing points.]{\label{TrainClouds1.figure:TestCloud}\includegraphics[width=2in,height=2in]{./Figures/TeastCloudAnootaion}} \\
%    \end{center}
%   \caption[Train set pointclouds]
%   {Point clouds used for extracting train samples with trash bin annotations. Bins are in different types and locations.(best viewed in color)}
%   \label{TrainClouds1.figure:edge}
% \end{figure}

\begin{figure} [htp]
     \begin{center}
    \subfigure[Full pointcloud of a kitchen.]{\label{TrainClouds1.figure:Kitchen4th}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Kitchen_4thFloor}}
    \subfigure[A partial view of a office.]{\label{TrainClouds1.figure:Office607}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Office_607_Desk}}\\
    \subfigure[A full pointcloud of an office.]{\label{TrainClouds1.figure:Office518}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Office_518}}
    \subfigure[A full pointcloud of another kitchen in the same building.]{\label{TrainClouds1.figure:Kitchen6th}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Kitchen_6thFloor}}
    \end{center}
  \caption[Train set pointclouds]
  {Point clouds used for extracting train samples with trash bin annotations. Bins are in different types and locations.(best viewed in color)}
  \label{TrainClouds1.figure:edge}
\end{figure}

In Figure \ref{TrainClouds1Challenge.figure:edge}, we see exmaples of pointclouds with missing points due to shadowing or presense of shiny object like a mirror. 

\begin{figure} [htp]
    \begin{center}
    \subfigure[A partial view of a bathroom.]{\label{TrainClouds1.figure:Bathroom}\includegraphics[width=2in,height=2in]{./Figures/CVAP_Bathroom_6thfloor}}
    \subfigure[A partial view of an office with lots of missing points.]{\label{TrainClouds1.figure:TestCloud}\includegraphics[width=2in,height=2in]{./Figures/TeastCloudAnootaion}}
   \end{center}
  \caption[Challenges with some pointclouds]
  {Point clouds with shodows and shiny surfaces (Mirror).(best viewed in color)}
  \label{TrainClouds1Challenge.figure:edge}
\end{figure}



Table \ref{Objects.table} includes names of four different object classes used in experiments and the number of train samples extracted for each of them.
It also shows what fraction of those samples were positive samples that are actually samples captured from context point blobs.


\begin{table}
\centering
\caption
[Object classes used in experiments.]
{Object classes used in experiments with number of samples extracted for each of them for training and ratio of positive samples in them.}
\label{Objects.table}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Object} & \#Train samples & \#Positive Train samples & Ratio (\%) \\
\hline
      1 & Trash Bin & 142150 & 2493 & 1.7 \\
\hline
      2 & Telephone   & 119725 & 554  & 0.4 \\
\hline
      3 & Mouse     & 286825 & 1541 & 0.5 \\
\hline
      4 & Wiper     & 170025 & 3086 & 1.8 \\
\hline

\end{tabular}
\end{table}


In experiments with four different object categories mentioned in table \ref{Objects.table}, a few different settings are considered, to make the train set more general. So instances are object are captured in different expected locations, hights, etc. Also other objects that can be in the scene together with the object of interest (which can be a part of their context) are considered in these different settings. Therefore, in the resulting train set each object has a number of instances in different scenes.
(figures \ref{TrainClouds2.figure:tmt1} and \ref{TrainClouds2.figure:tmt2})

\begin{figure} [htp]
   \begin{center}
    \subfigure[Partial cloud including telephone, mouse and trash bin.]{\label{TrainClouds2.figure:tmt1}\includegraphics[width=2in,height=2in]{./Figures/518_1_telefon_labels}}
    \subfigure[The same scene with different location of the objects.]{\label{TrainClouds2.figure:tmt2}\includegraphics[width=2in,height=2in]{./Figures/518_3_labels}} \\
    \subfigure[Partial cloud including white board wipers.]{\label{TrainClouds2.figure:wiper}\includegraphics[width=2in,height=2in]{./Figures/wiper_labels}}
    \subfigure[A partial cloud including several instances of 3 object categories.]{\label{TrainClouds2.figure:tmt3}\includegraphics[width=2in,height=2in]{./Figures/518_4_labels}} \\
  \end{center}
  \caption[Train set pointclouds including four different object class.]
  {Partial pointclouds including four object categories: Trash bin(marked with blue circles), mouse(back circles), telephone(red circles) and wiper(green circles) in different location settings used in training.(Best viewed in color)}
  \label{TrainClouds2.figure:edge}
\end{figure}




The aquired pointclouds then are taken through an automated workflow (python scripts) that picks a pointcloud from the input dataset, loads it into PreProcess module. Then the preprocessed pointclouds get annotated (labeling points belonging to objects of interest) in the Annotation tool.
Some of the annotated pointclouds are used for training and some for validation. 
For the ones used in validation labels from annotation do not get used until the end of classification, this information is needed for evaluation which would be described in the next Section (~\ref{Results.sec}).


In the next step, the annotated clouds and their normals (estimated in PreProcess) get loaded into Feature extract module 
and the resulting feature vectors are saved in a folder for the corresponding object class, under train or validation set, based on the set their source pointcloud belonged to.
Then, a subset is randomly selected from the train samples to make the train set. As mentioned before, the destribution of positive and negative samples is significantly havier for the negative ones. In first experiments for Trash bin we have used train sets with 10 percent positive and 90 percent negative samples.In later experiments and for other objects train sets are generated with all extracted positive samples and two times more of negative samples.In these experiments positive sample have a 33\% share in the train set. At this point, all data sets are scaled with the same parameters, then the scaled train set is given to SVM train to make the models for each of the object class's context. 


During training, different values for the parameters are considered, and each resulting model is applied on samples from validation
set. Classification results with different examined values of these parameters are compared and best models are selected. 
The results and evaluation procedure are discussed in Section ~\ref{Results.sec}.

\section{Results}
\label{Results.sec}
% Definitions and metrics
In this section, we take a look at some results and analyze and evaluate them to see how good our method performs.

\subsection{Evaluation metric}
\label{EvaluationMetric.ssec}

%As mentioned before there could be negative samples in train set that are labeled as negative just because they were not captured 
%from a neighborhood of an annotated object while they have very similar features to positive samples. 
%Such samples need to be predicted as positive. 

Here we define a specifc metric and evaluation method that we proposed and considered in this work.
Metrics such as accuracy of prediction, with their regular definition, are not the best ways to evaluate the performance of our system.
Accuracy is measuring number of samples that are correctly classified with respect to the total number of samples.
since the expected result is a correct likelihood map of the object context and not the object itself, we do not have direct access to the number of correct classification (The annotation labeled the object and not its context). 

Based on the problem definition, we are looking for possible context of an object, which is the regions in a scene where the 
object is expected to be found, not just regions where an instance of the object is actualy present. A table is a potential context for a cup, regardless of a cup being on it or not.   
Intuitively, a set of points that belong to an actual object (if there is an actual object in the scene) is a subset of all 
potentially candidate points for the object class, in that scene.
Therefore, actual object points or labeled points ($Lp$) are a subset of the predicted positive points ($Pp$) if the model and classifier perform 
well (equation \ref{Lp-PpRelation.eq}).

\begin{equation}
 \label{Lp-PpRelation.eq}
 Lp \subset Pp
\end{equation}

In other words, from a good result, if we randomly pick a point which belongs to an actual inastance of the object of interest, that point should be within the predicted positive points. A similar metric is used in the recently published and praised work \cite{aydemir2012_3Dcontext}.
%if the predictions give us points of the pointcloud where there is a high probability of finding the object, it
%would be what we want. 
%The point here is, that the location of the object would be a subset of this predicted set. 
With this definition there would be a problem: If all points get predicted as positive then actual object points would 
definitely be a subset of it, while our classification has failed.
To addres this issue, we define a good result as \textbf {predicted set of locations that reduces our search space for the object as much as 
possible while it preserves the high probability of finding the object}.

Equation ~\ref{Eval.eq} shows the evaluation metric, where $E(\tau)$ is the the value of the metric with respect to a 
probability threshold $\tau$. 
This threshold sets the boundary for the lowest probability value that the prediction for a point should have, to be 
considered as positive. 
%This threshold is assigned with a value in a loop that as a result in each iteration a top fraction of the samples are being 
%picked and analyzed. 
The threshold is computed through a tuning process where its value gets changed in a loop and for each threshold value,the probability of having the labled object within the positive predictions calculated. The optimal result would be a value for the threshold which results smallest subset of the pointcloud predicted as positive, while the chance of finding the object in the predicted region stays \%100. For instance, the threshold takes a value that separates the top five percent of the points with respect to their prediction value. Then, the probability of having the object in this fraction of points is computed.


$Pp_{\tau}$ is predicted positive sample with respect to the value of the threshold and $n(x)$ is the number of 
members in the set $x$, e.g. $n(Lp)$ it is number of points belonging to $Lp$. $Lp$ is the 
labeled positive points in the data-set or in other word the annotated object.

\begin{equation}
 \label{Eval.eq}
    E(\tau) = \frac{n({Pp_{\tau}} \cap {Lp})}{n(Lp)} 
\end{equation}

This value is between zero and one, and a value closer to one, while the threshold is high enough, shows a better result.
This metric is also used in experiments to find the best models and parameters. 
Here we also translate the sample base results into point base which means the result is assigned to the point for which 
the features are taken.
%Therefore, the prediction directly shows if a candidate OPoint is a possible object point or not.

%Figure ~\ref{Eval80ExperimentTop10.figure} shows the value of $E$ for top ten percent of the points with respect to their 
%prediction probability in experiments with 80 different configuration of weights and gamma.
%As it can be seen the results look pretty good.

% \begin{figure}[t]
%   \centerpsw{Eval80ExperimentTop10}{0.65\columnwidth}
%   \caption[Evaluation result of 80 experiments]
%   {Top 10 percent of positive predictions in experiments with 80 different configurations for parameters.}
%   \label{Eval80ExperimentTop10.figure}
% \end{figure}

\subsection{Evaluation results}
\label{EvaluationResult.ssec}

\begin{figure}[t]
  \centerpsw{evalMorevalues}{0.85\columnwidth}
  \caption[Evaluation diagram for experiments with Gaussian kernel.]
  {Evaluation diagram for experiments with Gaussian kernel. Each line corresponds to a value for w-1.(Best viewed in color.)}
  \label{evalSevenExp.figure}
\end{figure}
We have two sets of experiments, in the first one the learning is done using one Gaussian kernel on the whole feature vector.
In the second set we used double kernel of type chi-square with equal weights.
One is applied on first two field of the feature vector(orientation and height) and the other on the VFH part.

Figure ~\ref{evalSevenExp.figure} shows this evaluation for seven experiments with different values for w(-1) which is the 
parameter setting the weight of the negative class in SVM classifier.
In these experiments a single Gaussian kernel is employed to build the model of trash bin context.
The value is depicted with respect to the fraction of points considered.

Considering best models, it can be inferred from the curves that for instance, considering top five percent of the points 
contains twenty percent of the object which is a good result. 
In other words, reducing the the search space to only 5 percent we still have four time chance of finding the object.



Figure ~\ref{TrashPrediction.figure:edge} shows the prediction of trash bin context on a full pointcloud of an office. 
The marked region shows the points with highest probability values(top 1\%).
This predicted context reaches the trash bin which is present in the scene when the probability threshold reaches the 
probability of top 5\% of points.
The interesting point is that this trained model is suggesting that corner is the best location in this scene to find a 
trash bin or put one (based on the train data).

\begin{figure} [htp]
   \begin{center}
    \subfigure[A test pointcloud for trash bin model, marked region with red circle is the predicted context.]{\label{TrashPrediction.figure:full}\includegraphics[width=2.5in,height=2.5in]{./Figures/TrashBinPrediction}}
    \subfigure[Focused to predicted region.]{\label{TrashPrediction.figure:focused}\includegraphics[width=2.5in,height=2.5in]{./Figures/TrashBinPrediction_focused}} \\
  \end{center}
  \caption[Prediction of Trash bin model.]
  {Prediction of Trash bin context on a full pointcloud of an office. The marked region shows the points with highest probability values(top 1\%).(best viewed in color)}
  \label{TrashPrediction.figure:edge}
\end{figure}

In later experiments that we used multi kernel of type chi-square, there was a significant improvement on the results.
As the first two fields of our feature vector are two float values and the rest is histogram, they are to be treated 
in different ways.
So we separated the kernels applied on each of these parts.
The visualized results of predictions using these kernels with equal weights are shown in Figure ~\ref{ContextPrediction_518_1.figure:edge} 
for the four object we used in these experiments.

\begin{figure} [htp]
   \begin{center}
    \subfigure[Predicted context for Mouse.]{\label{ContextPrediction_518_1.figure:wiper}\includegraphics[width=2in,height=2in]{./Figures/518_1_MousePrediction_c1g0}}
    \subfigure[Predicted context for white-board wiper.]{\label{ContextPrediction_518_1.figure:mouse}\includegraphics[width=2in,height=2in]{./Figures/518_1_WiperPrediction_c1g0}} \\
    \subfigure[Predicted context for Telephone.]{\label{ContextPrediction_518_1.figure:Telephone}\includegraphics[width=2in,height=2in]{./Figures/518_1_TelefonPrediction_c1g0}}
    \subfigure[Predicted context for Trash Bin.]{\label{ContextPrediction_518_1.figure:TrashBin}\includegraphics[width=2in,height=2in]{./Figures/518_1_TrashBinPrediction_c1g0}} \\
  \end{center}
  \caption[Visualization of context model prediction on sample cloud 1 from validation set.]
  {Visualization of context model prediction on sample cloud 1 as a validation cloud, for four objects. The key point predicted as positive and the blob around it is projected on the pointcloud in cyan color.(best viewed in color)}
  \label{ContextPrediction_518_1.figure:edge}
\end{figure}

In Figure ~\ref{ContextPrediction_518_1.figure:edge}, the predicted contexts of four object used in experiments 
are projected on the pointcloud of an office.
It should be noticed that the regions shown in cyan, are blobs with a key point in the center which is predicted as 
context.

The corresponding evaluation values for results shown in Figure ~\ref{ContextPrediction_518_1.figure:edge} are depicted
in Figure ~\ref{Eval_518_1.figure}.
A simple comparison between curves in this figure and Figure ~\ref{evalSevenExp.figure} shows the improvement of the 
results.

\begin{figure}
  \begin{center}
   \centerpsw{Eval4Objects}{0.85\columnwidth}
  \caption[Evaluation results for four object classes.]
  {Evaluation results of context prediction for four objects Mouse, Telephone, Trash Bin and wiper. These results are from experiments using chi-square multi kernel.}
  \label{Eval_518_1.figure}
  \end{center}
\end{figure}

To compare the performance of the system regarding each of these four object class, the values of $E$ corresponding to each selected threshold (resulting in, a certain ratio of selected points with respect to all points in the pointcloud) is considered for each of these object classes.
Table ~\ref{performance.table} contains these values for the ratio of top 5\%.

\begin{table}
\centering
\caption
[Performance of the system for four object classes.]
{Performance of the system for four object classes.}
\label{performance.table}
\begin{tabular}{|c|c|c|}
\hline
Object & E in top 5\% of points & The ratio that E reaches to 1 \\
\hline
      Mouse & 0.6667 & 15 \\
\hline
      Telephone & 1 & 5 \\
\hline
      Trash Bin & 0.3333 &  40\\
\hline
      Wiper & 0.42 & 60 \\
\hline
\end{tabular}
\end{table}

As it can be seen in table ~\ref{performance.table}, the best performance of the system is for Telephone and after 
that for Mouse.
These two are often found in a similar context which can be a good reason for these results.
The context for trash bin, at least based on our train sets, can have wider variety compared to mouse and telephone.
That is why its performance value is the lowest.


In the wiper case, the reason for its low performance should be the wide range of heights this object can be located in. It can also be placed on a table.
We consider the height as an important feature in building our models which makes it very effective.
The effectiveness of height seems to be not so useful in the case of wiper.
A solution can be using smaller weights for the height feature in modeling these object classes.

\begin{figure} [htp]
   \begin{center}
    \subfigure[Predicted context for Mouse.]{\label{ContextPrediction_Test_512.figure:wiper}\includegraphics[width=2in,height=2in]{./Figures/Test21_2_13MousePredict_c1g0.eps}}
    \subfigure[Predicted context for white-board wiper.]{\label{ContextPrediction_Test_512.figure:mouse}\includegraphics[width=2in,height=2in]{./Figures/Test_wiperPrediction.eps}} \\
    \subfigure[Predicted context for Telephone.]{\label{ContextPrediction_Test_512.figure:Telephone}\includegraphics[width=2in,height=2in]{./Figures/Test_telefonPredict_c10g0.eps}}
    \subfigure[Predicted context for Trash Bin.]{\label{ContextPrediction_Test_512.figure:TrashBin}\includegraphics[width=2in,height=2in]{./Figures/Test_TrashBinPrediction_c1g0.eps}} \\
  \end{center}
  \caption[Visualization of context model prediction on sample cloud 2 as a test cloud.]
  {Visualization of context model prediction on sample cloud 2 as a test cloud for four objects. This pointcloud does not include 
  these objects except trash bin.(best viewed in color)}
  \label{ContextPrediction_Test_512.figure:edge}
\end{figure}

Figure ~\ref{ContextPrediction_Test_512.figure:edge} shows the predicted contexts of these four object class on the 
pointcloud of an office from test set.
In this test cloud no instance of wiper, mouse or telephone is present.
The reasonability of the predicted object contexts are visible in this figure despite absence of those instances.

\subsection*{Summery}
As mentioned in Section \ref{ExperimentalSetup.sec}, in the experiments we used pointclouds of different place categories like 
kitchen, bathroom and offices which included a number of instances of the four object category.
Each of these pointclouds in average contained around 8 million points out of which we extracted samples for a number of keypoints.
The average number of samples extracted from each pointcloud is approximately 50000.
Samples from several pointclouds are assembled in a set from which a train set is derived.
The set of clouds are divided into train (45\%), validation (15\%) and test(40\%).
Parameter and best model selection is done using validation set.

The results show that the predicted context is able to reduce the search space significantly while preserves the high chance of
finding the object of interest.
It also implies that the context of objects are modeled in a reliable way which makes it useful in place categorization where
we need to know just the possibility of presence of an object class in a query place (discussed in Chapter \ref{Conclusion.ch}). 
% (discussed in Chapter \ref{UsingObjectContextforPlaceClassification.ch}).
By tweaking the threshold of the prediction, the search space can be extended in a smart way rather than a greedy manner.

The time needed to train a model depends on the size of train set and the used parameters which in these experiments is in average
89 second.
Test takes approximately 45 second and the time needed for feature extraction per sample is 68 ms.


% 
% \begin{figure}[t]
%   \centerpsw{Eval80Experiment}{0.65\columnwidth}
%   \caption[]
%   {}
%   \label{}
% \end{figure}
% 
% 
% \begin{figure}[t]
%   \centerpsw{log(w1w-1)}{0.65\columnwidth}
%   \caption[]
%   {}
%   \label{}
% \end{figure}
% 
% \begin{figure}[t]
%   \centerpsw{PositiveRatio}{0.65\columnwidth}
%   \caption[]
%   {}
%   \label{}
% \end{figure}
% 
% \begin{figure}[t]
%   \centerpsw{TruePositiveRatio}{0.65\columnwidth}
%   \caption[]
%   {}
%   \label{}
% \end{figure}
% 
% \begin{figure}[t]
%   \centerpsw{w1w-1}{0.65\columnwidth}
%   \caption[]
%   {}
%   \label{}
% \end{figure}
% 
% 
